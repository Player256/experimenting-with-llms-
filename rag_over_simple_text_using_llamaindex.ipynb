{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOykptz9IgVpSsQuvZat8mL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Player256/experimenting-with-llms-/blob/main/rag_over_simple_text_using_llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RfKAzpju9pT"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests llama-index langchain"
      ],
      "metadata": {
        "id": "nlMwU6gPxNPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-embeddings-huggingface llama-index-llms-langchain\n"
      ],
      "metadata": {
        "id": "nNAvA_-wHevd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "model = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id, max_length=128, temperature=0.5, token=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "WEhr0K0GEsbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\"\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "  content = response.text\n",
        "\n",
        "  with open(\"/content/data/text_data.txt\", \"w\",encoding = \"utf-8\") as file :\n",
        "    file.write(content)\n",
        "  print(\"Data Extracted\")\n",
        "else : print(\"Failed to extract\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR_X4J86ySrF",
        "outputId": "715f151a-485a-4edc-cc04-0eb4ab2d7b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.core.embeddings import resolve_embed_model\n",
        "\n",
        "\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "\n",
        "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "\n",
        "Settings.llm = model\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        ")"
      ],
      "metadata": {
        "id": "2b9_nsgkFVAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What did your author do growing up?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJiBgvi-HEYr",
        "outputId": "b74b1180-7cba-4b9f-f2bd-6fb22bcc7068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The author grew up writing short stories and trying to write programs on an IBM 1401 computer. However, he found it difficult to figure out what to do with the computer as he didn't have any data to input. He was impressed by the ability of microcomputers to respond to keystrokes and began programming in earnest when he got a TRS-80 in about 1980. He wrote simple games, a word processor, and helped his father write a book. In college, he planned to study philosophy but found it boring and switched to Artificial Intelligence (AI) due to his fascination with intelligent computers depicted in a novel and a documentary. He and a friend started working on a new dialect of Lisp, which led to his first talk at a Lisp conference and the discovery of an online audience for his writing. He continued to work on various projects, including essays, and eventually met his future wife at a party in 2003.\n"
          ]
        }
      ]
    }
  ]
}